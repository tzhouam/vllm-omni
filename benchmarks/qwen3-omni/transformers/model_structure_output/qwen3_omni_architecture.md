# Qwen3 Omni 30B æ¨¡å‹æ¶æ„

å®Œæ•´çš„ Qwen3 Omni 30B æ¨¡å‹æ¶æ„,å±•ç¤ºæ•°æ®æµã€å½¢çŠ¶å’Œç±»å‹ã€‚

## å®Œæ•´æ¶æ„æµç¨‹å›¾

```mermaid
flowchart TD
    %% Input
    Input["è¾“å…¥æ–‡æœ¬ Token<br/>Input:  1x14 int64<br/>Output: 1x14 int64"]
    
    %% ===== THINKER éƒ¨åˆ† (æ€ç»´æ¨¡å—) =====
    subgraph Thinker["ğŸ§  Thinker (å¤šæ¨¡æ€ç†è§£ + æ–‡æœ¬ç”Ÿæˆ)<br/>Input:  1x14 int64<br/>Output1: 1x14x2048 bf16 (hidden)<br/>Output2: 1x14x152064 bf16 (logits)"]
        T1["Embedding Layer<br/>[1, 14] int64<br/>â†“<br/>[1, 14, 2048] bf16"]
        T2["Rotary Embedding<br/>RoPE ä½ç½®ç¼–ç <br/>[1, 14, 2048] bf16"]
        
        subgraph T_Layers["48 Ã— Decoder Layer (MoE)"]
            TL1["Layer N"]
            TL1_LN1["RMSNorm<br/>[1, seq, 2048] bf16"]
            TL1_Attn["Self Attention<br/>å¤šå¤´æ³¨æ„åŠ›<br/>[1, seq, 2048] bf16"]
            TL1_LN2["RMSNorm<br/>[1, seq, 2048] bf16"]
            TL1_MoE["Sparse MoE Block<br/>ä¸“å®¶æ··åˆå±‚<br/>128 experts<br/>[1, seq, 2048] bf16"]
            
            TL1_LN1 --> TL1_Attn
            TL1_Attn --> TL1_LN2
            TL1_LN2 --> TL1_MoE
        end
        
        T3["Final RMSNorm<br/>[1, seq, 2048] bf16"]
        T4["LM Head (Linear)<br/>[1, seq, 2048] bf16<br/>â†“<br/>[1, seq, 152064] bf16<br/>è¯è¡¨å¤§å°: 152064"]
        
        T1 --> T2
        T2 --> T_Layers
        T_Layers --> T3
        T3 --> T4
    end
    
    %% ===== BRIDGE æ¡¥æ¥å±‚ =====
    Bridge["ğŸ”— Text Projection (ResizeMLP)<br/>Linear(2048â†’2048) + SiLU â†’ Linear(2048â†’1024)<br/>Input:  1x20x2048 bf16 (ç¤ºä¾‹)<br/>Output: 1x20x1024 bf16 (ç¤ºä¾‹)"]
    
    %% ===== TALKER éƒ¨åˆ† (è¯­éŸ³ç”Ÿæˆ) =====
    subgraph Talker["ğŸ—£ï¸ Talker (è¯­éŸ³ç”Ÿæˆ)<br/>Input:  1x20x1024 bf16 (ç¤ºä¾‹)<br/>Output: 1x16x39 int64 (RVQ codes, ç¤ºä¾‹)"]
        direction TB
        
        subgraph Talker_LM["Talker Language Model<br/>Input:  1x20x1024 bf16 (ç¤ºä¾‹)<br/>Output: 1x20x1024 bf16 (ç¤ºä¾‹)"]
            TA1["Codec Embedding<br/>[1, 6] int64<br/>â†“<br/>[1, 6, 1024] bf16"]
            TA2["Rotary Embedding<br/>[1, 20, 1024] bf16"]
            
            subgraph TA_Layers["48 Ã— Decoder Layer (MoE)"]
                TAL1["Layer N"]
                TAL1_LN1["RMSNorm<br/>[1, seq, 1024] bf16"]
                TAL1_Attn["Self Attention<br/>[1, seq, 1024] bf16"]
                TAL1_LN2["RMSNorm<br/>[1, seq, 1024] bf16"]
                TAL1_MoE["Sparse MoE Block<br/>128 experts<br/>[1, seq, 1024] bf16"]
                
                TAL1_LN1 --> TAL1_Attn
                TAL1_Attn --> TAL1_LN2
                TAL1_LN2 --> TAL1_MoE
            end
            
            TA3["Final RMSNorm<br/>[1, seq, 1024] bf16"]
            
            TA1 --> TA2
            TA2 --> TA_Layers
            TA_Layers --> TA3
        end
        
        subgraph CodePredictor["Code Predictor (å¤šç æœ¬é¢„æµ‹)<br/>Input:  1x1x1024 bf16 (ç¤ºä¾‹)<br/>Output: 1x1x2048 bf16 (logits, ç¤ºä¾‹) â†’ sample â†’ int64 code"]
            direction TB
            CP1["16 Ã— Codec Heads<br/>æ¯ä¸ªç æœ¬ç‹¬ç«‹é¢„æµ‹"]
            
            subgraph CP_Single["å•ä¸ª Codec Head"]
                CP_Emb["Codec Embedding<br/>[1, 1] int64<br/>â†“<br/>[1, 1, 1024] bf16"]
                CP_RoPE["Rotary Embedding"]
                
                subgraph CP_Layers["5 Ã— Decoder Layer"]
                    CPL1["Layer N"]
                    CPL1_LN1["RMSNorm"]
                    CPL1_Attn["Self Attention"]
                    CPL1_LN2["RMSNorm"]
                    CPL1_MLP["MLP"]
                    
                    CPL1_LN1 --> CPL1_Attn
                    CPL1_Attn --> CPL1_LN2
                    CPL1_LN2 --> CPL1_MLP
                end
                
                CP_Norm["RMSNorm"]
                CP_Head["LM Head (Linear)<br/>[1, 1, 1024] bf16<br/>â†“<br/>[1, 1, 2048] bf16"]
                
                CP_Emb --> CP_RoPE
                CP_RoPE --> CP_Layers
                CP_Layers --> CP_Norm
                CP_Norm --> CP_Head
            end
            
            CP1 --> CP_Single
        end
        
        Talker_LM --> CodePredictor
    end
    
    %% ===== CODE2WAV éƒ¨åˆ† (éŸ³é¢‘è§£ç ) =====
    subgraph Code2Wav["ğŸµ Code2Wav (Vocoder å£°ç å™¨)<br/>Input:  1x16x39 int64 (RVQ codes)<br/>Output: 1xT_audio float32 (waveform)"]
        direction TB
        C1["Code Embedding<br/>[1, 16, 39] int64<br/>â†“<br/>[1, 16, 39, 1024] bf16<br/>16ä¸ªç æœ¬, å…±39ä¸ªcodes"]
        
        subgraph C_PreTrans["Pre Transformer"]
            C2["Rotary Embedding<br/>[1, 39, 1024] bf16"]
            
            subgraph C_PreLayers["18 Ã— Transformer Layer"]
                CL1["Layer N"]
                CL1_LN1["RMSNorm"]
                CL1_Attn["Self Attention"]
                CL1_Scale1["Layer Scale"]
                CL1_LN2["RMSNorm"]
                CL1_MLP["MLP"]
                CL1_Scale2["Layer Scale"]
                
                CL1_LN1 --> CL1_Attn
                CL1_Attn --> CL1_Scale1
                CL1_Scale1 --> CL1_LN2
                CL1_LN2 --> CL1_MLP
                CL1_MLP --> CL1_Scale2
            end
            
            C3["Final RMSNorm<br/>[1, 39, 1024] bf16"]
            
            C2 --> C_PreLayers
            C_PreLayers --> C3
        end
        
        subgraph C_PostTrans["Post Transformer"]
            C4["Rotary Embedding"]
            
            subgraph C_PostLayers["18 Ã— Transformer Layer"]
                CPL["ç±»ä¼¼ Pre Transformer ç»“æ„"]
            end
            
            C5["Final RMSNorm"]
            C4 --> C_PostLayers
            C_PostLayers --> C5
        end
        
        C6["Conv Layers<br/>ä¸Šé‡‡æ · + å·ç§¯"]
        C7["Wave Output<br/>éŸ³é¢‘æ³¢å½¢<br/>[1, time_steps]<br/>float32"]
        
        C1 --> C_PreTrans
        C_PreTrans --> C_PostTrans
        C_PostTrans --> C6
        C6 --> C7
    end
    
    %% ===== æ•°æ®æµè¿æ¥ =====
    Input --> Thinker
    Thinker --> Bridge
    Bridge --> Talker
    Talker --> Code2Wav
    Code2Wav --> Output["éŸ³é¢‘è¾“å‡º<br/>Waveform"]
    
    %% æ ·å¼å®šä¹‰
    classDef thinkerClass fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef talkerClass fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    classDef code2wavClass fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px
    classDef bridgeClass fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    classDef ioClass fill:#ffebee,stroke:#c62828,stroke-width:2px
    
    class T1,T2,T_Layers,T3,T4,TL1,TL1_LN1,TL1_Attn,TL1_LN2,TL1_MoE thinkerClass
    class TA1,TA2,TA_Layers,TA3,TAL1,TAL1_LN1,TAL1_Attn,TAL1_LN2,TAL1_MoE talkerClass
    class CP1,CP_Single,CP_Emb,CP_RoPE,CP_Layers,CP_Norm,CP_Head,CPL1,CPL1_LN1,CPL1_Attn,CPL1_LN2,CPL1_MLP talkerClass
    class C1,C2,C_PreLayers,C3,C4,C_PostLayers,C5,C6,C7,CL1,CL1_LN1,CL1_Attn,CL1_Scale1,CL1_LN2,CL1_MLP,CL1_Scale2,CPL code2wavClass
    class Bridge bridgeClass
    class Input,Output ioClass
```

## æ¨¡å‹è¯¦ç»†è§„æ ¼

### 1. Thinker (æ€ç»´æ¨¡å—)
- **åŠŸèƒ½**: å¤šæ¨¡æ€ç†è§£å’Œæ–‡æœ¬ç”Ÿæˆ
- **å±‚æ•°**: 48 å±‚ Transformer Decoder
- **éšè—ç»´åº¦**: 2048
- **MoE é…ç½®**: 
  - æ¯å±‚ 128 ä¸ªä¸“å®¶
  - ç¨€ç–æ¿€æ´»(åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶)
- **è¾“å…¥**: æ–‡æœ¬ token (int64)
- **è¾“å‡º**: æ–‡æœ¬logits [vocab_size=152064]
- **æ•°æ®ç±»å‹**: bfloat16

**æ¯å±‚ç»“æ„**:
```
è¾“å…¥ â†’ RMSNorm â†’ Self-Attention â†’ æ®‹å·®è¿æ¥
     â†“
     â†’ RMSNorm â†’ Sparse MoE Block â†’ æ®‹å·®è¿æ¥ â†’ è¾“å‡º
```

### 2. Text Projection (æ¡¥æ¥å±‚)
- **åŠŸèƒ½**: å°† Thinker çš„ 2048 ç»´é™åˆ° Talker çš„ 1024 ç»´
- **ç»“æ„**: 
  - Linear(2048 â†’ 2048)
  - SiLU æ¿€æ´»
  - Linear(2048 â†’ 1024)
- **æ•°æ®ç±»å‹**: bfloat16

### 3. Talker (è¯­éŸ³ç”Ÿæˆæ¨¡å—)

#### 3.1 Talker Language Model
- **å±‚æ•°**: 48 å±‚ Transformer Decoder
- **éšè—ç»´åº¦**: 1024
- **MoE é…ç½®**: æ¯å±‚ 128 ä¸ªä¸“å®¶
- **è¾“å…¥**: æ¥è‡ª Thinker çš„æŠ•å½± + Codec embeddings
- **è¾“å‡º**: ä¸­é—´è¡¨ç¤ºç”¨äº Code Predictor

#### 3.2 Code Predictor (ç æœ¬é¢„æµ‹å™¨)
- **åŠŸèƒ½**: å¤šç æœ¬é¢„æµ‹(Multi-Token Prediction)
- **ç æœ¬æ•°é‡**: 16 ä¸ªç‹¬ç«‹ç æœ¬
- **æ¯ä¸ªç æœ¬**:
  - 5 å±‚ Transformer Decoder
  - éšè—ç»´åº¦: 1024
  - è¾“å‡º: 2048 ä¸ª codes
- **æ•°æ®ç±»å‹**: bfloat16

### 4. Code2Wav (å£°ç å™¨)
- **åŠŸèƒ½**: å°† RVQ codes è½¬æ¢ä¸ºéŸ³é¢‘æ³¢å½¢
- **è¾“å…¥**: 16 ä¸ªç æœ¬ Ã— 39 ä¸ª codes/å¸§
- **ç»“æ„**:
  - **Pre Transformer**: 18 å±‚
    - RMSNorm + Self-Attention + Layer Scale
    - RMSNorm + MLP + Layer Scale
  - **Post Transformer**: 18 å±‚(ç›¸åŒç»“æ„)
  - **Wave Conv**: ä¸Šé‡‡æ ·å·ç§¯å±‚
- **è¾“å‡º**: éŸ³é¢‘æ³¢å½¢ (float32)
- **æ•°æ®ç±»å‹**: bfloat16 (transformer), float32 (output)

## å…³é”®ç‰¹æ€§

### MoE (Mixture of Experts)
- **ä¸“å®¶æ•°é‡**: 128 ä¸ªä¸“å®¶/å±‚
- **æ¿€æ´»ç­–ç•¥**: ç¨€ç–æ¿€æ´»
- **ä½ç½®**: Thinker å’Œ Talker çš„æ¯ä¸ª MLP å±‚

### æ³¨æ„åŠ›æœºåˆ¶
- **ç±»å‹**: Multi-Head Self-Attention
- **ä½ç½®ç¼–ç **: RoPE (Rotary Position Embedding)
- **KV Cache**: æ”¯æŒå¢é‡è§£ç 

### å½’ä¸€åŒ–
- **ç±»å‹**: RMSNorm (Root Mean Square Layer Normalization)
- **ä½ç½®**: æ¯ä¸ª attention/MLP ä¹‹å‰

### æ•°æ®ç±»å‹ä¼˜åŒ–
- **ä¸»è¦è®¡ç®—**: bfloat16 (é™ä½æ˜¾å­˜,åŠ é€Ÿè®¡ç®—)
- **è¾“å…¥ tokens**: int64
- **éŸ³é¢‘è¾“å‡º**: float32 (ä¿è¯è´¨é‡)

## æ•°æ®æµæ€»è§ˆ

```
æ–‡æœ¬è¾“å…¥ (int64)
    â†“
Thinker Embedding (2048-dim, bf16)
    â†“
48 Ã— Decoder Layer (MoE)
    â†“
LM Head â†’ æ–‡æœ¬ logits
    â†“
Text Projection (2048â†’1024)
    â†“
Talker LM (48 Ã— Decoder Layer, MoE)
    â†“
Code Predictor (16 Ã— 5-layer, é¢„æµ‹ RVQ codes)
    â†“
Code2Wav Pre-Transformer (18å±‚)
    â†“
Code2Wav Post-Transformer (18å±‚)
    â†“
Wave Conv (ä¸Šé‡‡æ ·)
    â†“
éŸ³é¢‘æ³¢å½¢è¾“å‡º (float32)
```

## æ¨¡å‹è§„æ¨¡

- **æ€»å‚æ•°**: ~30B
- **Thinker**: ~20B (48å±‚ Ã— 2048-dim MoE)
- **Talker**: ~8B (48å±‚ Ã— 1024-dim MoE + Code Predictor)
- **Code2Wav**: ~2B (36å±‚ transformer + conv)

## ä¸»è¦ä¼˜åŒ–

1. **MoE æ¶æ„**: å¤§å®¹é‡å‚æ•°,å®é™…æ¿€æ´»å‚æ•°è¾ƒå°‘
2. **bfloat16**: é™ä½æ˜¾å­˜å ç”¨,åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†
3. **å¤šé˜¶æ®µè®¾è®¡**: Thinkerâ†’Talkerâ†’Code2Wav æµæ°´çº¿
4. **å¤šç æœ¬é¢„æµ‹**: 16ä¸ªç æœ¬å¹¶è¡Œé¢„æµ‹,æé«˜éŸ³é¢‘è´¨é‡

---

*ç”Ÿæˆæ—¶é—´: 2026-01-09*
*åŸºäº Qwen3-Omni-30B-A3B-Thinking æ¨¡å‹*
